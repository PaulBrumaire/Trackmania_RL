Training for 3000 steps ...
WARNING:tensorflow:From C:\Users\quent\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
  100/3000: episode: 1, duration: 14.132s, episode steps: 100, steps per second:   7, episode reward: -12.000, mean reward: -0.120 [-1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: --, mae: --, mean_q: --
  200/3000: episode: 2, duration: 14.881s, episode steps: 100, steps per second:   7, episode reward: 34.000, mean reward:  0.340 [-1.000,  1.000], mean action: 0.330 [0.000, 1.000],  loss: 0.285121, mae: 0.539775, mean_q: 0.943525
  288/3000: episode: 3, duration: 14.895s, episode steps:  88, steps per second:   6, episode reward: 1262.000, mean reward: 14.341 [-1.000, 1201.000], mean action: 0.148 [0.000, 1.000],  loss: 0.300604, mae: 0.512301, mean_q: 0.998469
  388/3000: episode: 4, duration: 14.699s, episode steps: 100, steps per second:   7, episode reward: 58.000, mean reward:  0.580 [-1.000,  1.000], mean action: 0.210 [0.000, 1.000],  loss: 0.342459, mae: 0.513842, mean_q: 0.999609
  488/3000: episode: 5, duration: 14.676s, episode steps: 100, steps per second:   7, episode reward: 36.000, mean reward:  0.360 [-1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 0.347890, mae: 0.503844, mean_q: 0.999702
  588/3000: episode: 6, duration: 14.613s, episode steps: 100, steps per second:   7, episode reward: 44.000, mean reward:  0.440 [-1.000,  1.000], mean action: 0.280 [0.000, 1.000],  loss: 0.372383, mae: 0.497761, mean_q: 0.999897
  688/3000: episode: 7, duration: 14.688s, episode steps: 100, steps per second:   7, episode reward: 54.000, mean reward:  0.540 [-1.000,  1.000], mean action: 0.230 [0.000, 1.000],  loss: 0.357732, mae: 0.498142, mean_q: 0.999912
  788/3000: episode: 8, duration: 14.781s, episode steps: 100, steps per second:   7, episode reward: 40.000, mean reward:  0.400 [-1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.333351, mae: 0.498455, mean_q: 0.999959
  888/3000: episode: 9, duration: 14.535s, episode steps: 100, steps per second:   7, episode reward: 34.000, mean reward:  0.340 [-1.000,  1.000], mean action: 0.330 [0.000, 1.000],  loss: 0.343049, mae: 0.483091, mean_q: 0.999968
  988/3000: episode: 10, duration: 14.666s, episode steps: 100, steps per second:   7, episode reward: 40.000, mean reward:  0.400 [-1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 7200.308594, mae: 6.493814, mean_q: 0.999982
 1088/3000: episode: 11, duration: 14.669s, episode steps: 100, steps per second:   7, episode reward: 32.000, mean reward:  0.320 [-1.000,  1.000], mean action: 0.340 [0.000, 1.000],  loss: 0.357745, mae: 0.482811, mean_q: 0.999973
 1188/3000: episode: 12, duration: 14.752s, episode steps: 100, steps per second:   7, episode reward: 64.000, mean reward:  0.640 [-1.000,  1.000], mean action: 0.180 [0.000, 1.000],  loss: 0.367551, mae: 0.497786, mean_q: 0.999976
 1288/3000: episode: 13, duration: 14.691s, episode steps: 100, steps per second:   7, episode reward: 62.000, mean reward:  0.620 [-1.000,  1.000], mean action: 0.190 [0.000, 1.000],  loss: 14400.299805, mae: 12.488903, mean_q: 0.999992
 1388/3000: episode: 14, duration: 14.670s, episode steps: 100, steps per second:   7, episode reward: 46.000, mean reward:  0.460 [-1.000,  1.000], mean action: 0.270 [0.000, 1.000],  loss: 0.367552, mae: 0.497662, mean_q: 0.999988
 1488/3000: episode: 15, duration: 14.774s, episode steps: 100, steps per second:   7, episode reward: 48.000, mean reward:  0.480 [-1.000,  1.000], mean action: 0.260 [0.000, 1.000],  loss: 0.338151, mae: 0.493186, mean_q: 0.999992
 1588/3000: episode: 16, duration: 14.619s, episode steps: 100, steps per second:   7, episode reward: 54.000, mean reward:  0.540 [-1.000,  1.000], mean action: 0.230 [0.000, 1.000],  loss: 0.289152, mae: 0.499261, mean_q: 0.999991
 1688/3000: episode: 17, duration: 14.528s, episode steps: 100, steps per second:   7, episode reward: 34.000, mean reward:  0.340 [-1.000,  1.000], mean action: 0.330 [0.000, 1.000],  loss: 0.343050, mae: 0.487992, mean_q: 0.999995
 1788/3000: episode: 18, duration: 14.549s, episode steps: 100, steps per second:   7, episode reward: 50.000, mean reward:  0.500 [-1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.308749, mae: 0.488658, mean_q: 0.999997
 1888/3000: episode: 19, duration: 14.623s, episode steps: 100, steps per second:   7, episode reward: 38.000, mean reward:  0.380 [-1.000,  1.000], mean action: 0.310 [0.000, 1.000],  loss: 0.338151, mae: 0.498227, mean_q: 0.999994
 1988/3000: episode: 20, duration: 14.600s, episode steps: 100, steps per second:   7, episode reward: 54.000, mean reward:  0.540 [-1.000,  1.000], mean action: 0.230 [0.000, 1.000],  loss: 0.357750, mae: 0.492733, mean_q: 0.999996
 2088/3000: episode: 21, duration: 14.709s, episode steps: 100, steps per second:   7, episode reward: 46.000, mean reward:  0.460 [-1.000,  1.000], mean action: 0.270 [0.000, 1.000],  loss: 0.348051, mae: 0.503026, mean_q: 0.999997
 2188/3000: episode: 22, duration: 14.642s, episode steps: 100, steps per second:   7, episode reward: 42.000, mean reward:  0.420 [-1.000,  1.000], mean action: 0.290 [0.000, 1.000],  loss: 0.308751, mae: 0.498756, mean_q: 0.999998
 2288/3000: episode: 23, duration: 14.699s, episode steps: 100, steps per second:   7, episode reward: 46.000, mean reward:  0.460 [-1.000,  1.000], mean action: 0.270 [0.000, 1.000],  loss: 0.313650, mae: 0.498636, mean_q: 0.999998
 2388/3000: episode: 24, duration: 14.618s, episode steps: 100, steps per second:   7, episode reward: 36.000, mean reward:  0.360 [-1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 0.338149, mae: 0.488042, mean_q: 0.999998
 2488/3000: episode: 25, duration: 14.714s, episode steps: 100, steps per second:   7, episode reward: 60.000, mean reward:  0.600 [-1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.372450, mae: 0.497465, mean_q: 0.999997
 2588/3000: episode: 26, duration: 14.667s, episode steps: 100, steps per second:   7, episode reward: 40.000, mean reward:  0.400 [-1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.333350, mae: 0.498238, mean_q: 0.999999
 2688/3000: episode: 27, duration: 14.710s, episode steps: 100, steps per second:   7, episode reward: 46.000, mean reward:  0.460 [-1.000,  1.000], mean action: 0.270 [0.000, 1.000],  loss: 0.348050, mae: 0.503009, mean_q: 0.999998
 2788/3000: episode: 28, duration: 14.708s, episode steps: 100, steps per second:   7, episode reward: 44.000, mean reward:  0.440 [-1.000,  1.000], mean action: 0.280 [0.000, 1.000],  loss: 0.347950, mae: 0.492891, mean_q: 0.999999
 2888/3000: episode: 29, duration: 14.703s, episode steps: 100, steps per second:   7, episode reward: 36.000, mean reward:  0.360 [-1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 0.318550, mae: 0.493538, mean_q: 0.999997
 2988/3000: episode: 30, duration: 14.503s, episode steps: 100, steps per second:   7, episode reward: 42.000, mean reward:  0.420 [-1.000,  1.000], mean action: 0.290 [0.000, 1.000],  loss: 0.362650, mae: 0.497638, mean_q: 0.999999
done, took 441.819 seconds