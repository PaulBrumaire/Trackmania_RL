Training for 100000 steps ...
   400/100000: episode: 1, duration: 32.817s, episode steps: 400, steps per second:  12, episode reward: 1889.812, mean reward:  4.725 [ 0.000, 25.352], mean action: 1.552 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   800/100000: episode: 2, duration: 32.561s, episode steps: 400, steps per second:  12, episode reward: 2276.738, mean reward:  5.692 [ 0.000, 30.394], mean action: 1.542 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  1200/100000: episode: 3, duration: 32.622s, episode steps: 400, steps per second:  12, episode reward: 1906.220, mean reward:  4.766 [ 0.000, 21.005], mean action: 1.520 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  1600/100000: episode: 4, duration: 33.016s, episode steps: 400, steps per second:  12, episode reward: 2107.594, mean reward:  5.269 [ 0.000, 20.088], mean action: 1.445 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  2000/100000: episode: 5, duration: 33.014s, episode steps: 400, steps per second:  12, episode reward: 1920.146, mean reward:  4.800 [ 0.000, 25.703], mean action: 1.530 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  2400/100000: episode: 6, duration: 34.964s, episode steps: 400, steps per second:  11, episode reward: 2594.974, mean reward:  6.487 [ 0.000, 28.899], mean action: 1.435 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  2800/100000: episode: 7, duration: 32.616s, episode steps: 400, steps per second:  12, episode reward: 1839.583, mean reward:  4.599 [ 0.000, 27.430], mean action: 1.538 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  3200/100000: episode: 8, duration: 32.685s, episode steps: 400, steps per second:  12, episode reward: 3581.093, mean reward:  8.953 [ 0.000, 39.436], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  3600/100000: episode: 9, duration: 32.659s, episode steps: 400, steps per second:  12, episode reward: 2422.863, mean reward:  6.057 [ 0.000, 28.137], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  4000/100000: episode: 10, duration: 32.587s, episode steps: 400, steps per second:  12, episode reward: 1903.432, mean reward:  4.759 [ 0.000, 29.483], mean action: 1.490 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  4400/100000: episode: 11, duration: 32.615s, episode steps: 400, steps per second:  12, episode reward: 2085.788, mean reward:  5.214 [ 0.000, 21.531], mean action: 1.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  4800/100000: episode: 12, duration: 32.548s, episode steps: 400, steps per second:  12, episode reward: 1952.525, mean reward:  4.881 [ 0.000, 26.325], mean action: 1.415 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
  5200/100000: episode: 13, duration: 33.791s, episode steps: 400, steps per second:  12, episode reward: 2107.092, mean reward:  5.268 [ 0.000, 29.021], mean action: 1.542 [0.000, 3.000],  loss: 96.263778, mae: 113.255323, mean_q: 167.327511, mean_eps: 0.954100
  5600/100000: episode: 14, duration: 33.390s, episode steps: 400, steps per second:  12, episode reward: 3154.227, mean reward:  7.886 [ 0.000, 28.002], mean action: 1.403 [0.000, 3.000],  loss: 46.893450, mae: 109.475721, mean_q: 173.769876, mean_eps: 0.951404
  6000/100000: episode: 15, duration: 33.364s, episode steps: 400, steps per second:  12, episode reward: 1863.830, mean reward:  4.660 [ 0.000, 19.249], mean action: 1.583 [0.000, 3.000],  loss: 11.408455, mae: 128.197369, mean_q: 178.109873, mean_eps: 0.947805
  6400/100000: episode: 16, duration: 32.937s, episode steps: 400, steps per second:  12, episode reward: 1924.897, mean reward:  4.812 [ 0.000, 19.187], mean action: 1.623 [0.000, 3.000],  loss: 9.414780, mae: 134.818849, mean_q: 182.748057, mean_eps: 0.944204
  6800/100000: episode: 17, duration: 33.308s, episode steps: 400, steps per second:  12, episode reward: 2523.406, mean reward:  6.309 [ 0.000, 33.202], mean action: 1.528 [0.000, 3.000],  loss: 10.674485, mae: 134.298609, mean_q: 182.075258, mean_eps: 0.940605
  7200/100000: episode: 18, duration: 33.309s, episode steps: 400, steps per second:  12, episode reward: 2641.749, mean reward:  6.604 [ 0.000, 43.126], mean action: 1.525 [0.000, 3.000],  loss: 8.619195, mae: 137.228303, mean_q: 186.054558, mean_eps: 0.937004
  7600/100000: episode: 19, duration: 33.038s, episode steps: 400, steps per second:  12, episode reward: 2465.359, mean reward:  6.163 [ 0.000, 25.050], mean action: 1.560 [0.000, 3.000],  loss: 8.541698, mae: 138.203481, mean_q: 188.630990, mean_eps: 0.933405
  8000/100000: episode: 20, duration: 33.296s, episode steps: 400, steps per second:  12, episode reward: 2940.664, mean reward:  7.352 [ 0.000, 34.556], mean action: 1.462 [0.000, 3.000],  loss: 8.741238, mae: 138.645695, mean_q: 189.250696, mean_eps: 0.929804
  8400/100000: episode: 21, duration: 34.913s, episode steps: 400, steps per second:  11, episode reward: 2675.440, mean reward:  6.689 [ 0.000, 22.748], mean action: 1.290 [0.000, 3.000],  loss: 9.501012, mae: 143.974505, mean_q: 194.606157, mean_eps: 0.926205
  8800/100000: episode: 22, duration: 37.451s, episode steps: 400, steps per second:  11, episode reward: 2492.715, mean reward:  6.232 [ 0.000, 28.243], mean action: 1.510 [0.000, 3.000],  loss: 8.398617, mae: 143.826742, mean_q: 195.889481, mean_eps: 0.922604
  9200/100000: episode: 23, duration: 33.492s, episode steps: 400, steps per second:  12, episode reward: 1887.426, mean reward:  4.719 [ 0.000, 23.245], mean action: 1.542 [0.000, 3.000],  loss: 8.783605, mae: 144.944051, mean_q: 197.044828, mean_eps: 0.919005
  9600/100000: episode: 24, duration: 33.006s, episode steps: 400, steps per second:  12, episode reward: 2025.539, mean reward:  5.064 [ 0.000, 21.369], mean action: 1.562 [0.000, 3.000],  loss: 8.404435, mae: 145.989730, mean_q: 198.725064, mean_eps: 0.915404
 10000/100000: episode: 25, duration: 33.245s, episode steps: 400, steps per second:  12, episode reward: 2608.671, mean reward:  6.522 [ 0.000, 21.678], mean action: 1.488 [0.000, 3.000],  loss: 9.265094, mae: 146.412503, mean_q: 199.572507, mean_eps: 0.911805
 10400/100000: episode: 26, duration: 33.145s, episode steps: 400, steps per second:  12, episode reward: 2130.711, mean reward:  5.327 [ 0.000, 31.786], mean action: 1.548 [0.000, 3.000],  loss: 8.566511, mae: 153.073797, mean_q: 208.484958, mean_eps: 0.908204
 10800/100000: episode: 27, duration: 33.221s, episode steps: 400, steps per second:  12, episode reward: 2013.614, mean reward:  5.034 [ 0.000, 26.562], mean action: 1.423 [0.000, 3.000],  loss: 9.948790, mae: 150.673656, mean_q: 205.041444, mean_eps: 0.904605
 11200/100000: episode: 28, duration: 33.069s, episode steps: 400, steps per second:  12, episode reward: 2112.223, mean reward:  5.281 [ 0.000, 21.161], mean action: 1.510 [0.000, 3.000],  loss: 8.851092, mae: 153.452848, mean_q: 208.891488, mean_eps: 0.901004
 11600/100000: episode: 29, duration: 33.502s, episode steps: 400, steps per second:  12, episode reward: 2237.849, mean reward:  5.595 [ 0.000, 21.174], mean action: 1.542 [0.000, 3.000],  loss: 10.332448, mae: 155.856230, mean_q: 212.315961, mean_eps: 0.897404
 12000/100000: episode: 30, duration: 33.160s, episode steps: 400, steps per second:  12, episode reward: 2201.377, mean reward:  5.503 [ 0.000, 19.583], mean action: 1.600 [0.000, 3.000],  loss: 8.046055, mae: 155.952510, mean_q: 212.304842, mean_eps: 0.893805
 12400/100000: episode: 31, duration: 33.254s, episode steps: 400, steps per second:  12, episode reward: 2539.726, mean reward:  6.349 [ 0.000, 24.527], mean action: 1.465 [0.000, 3.000],  loss: 9.184033, mae: 159.863236, mean_q: 217.352345, mean_eps: 0.890204
 12800/100000: episode: 32, duration: 33.047s, episode steps: 400, steps per second:  12, episode reward: 2398.962, mean reward:  5.997 [ 0.000, 33.325], mean action: 1.567 [0.000, 3.000],  loss: 9.134267, mae: 160.592214, mean_q: 218.570787, mean_eps: 0.886605
 13200/100000: episode: 33, duration: 33.232s, episode steps: 400, steps per second:  12, episode reward: 3743.248, mean reward:  9.358 [ 0.000, 28.044], mean action: 1.442 [0.000, 3.000],  loss: 8.873358, mae: 163.094153, mean_q: 221.832018, mean_eps: 0.883005
 13600/100000: episode: 34, duration: 33.318s, episode steps: 400, steps per second:  12, episode reward: 2451.202, mean reward:  6.128 [ 0.000, 24.850], mean action: 1.498 [0.000, 3.000],  loss: 10.063426, mae: 165.745808, mean_q: 225.491872, mean_eps: 0.879405
 14000/100000: episode: 35, duration: 33.030s, episode steps: 400, steps per second:  12, episode reward: 2796.106, mean reward:  6.990 [ 0.000, 34.097], mean action: 1.577 [0.000, 3.000],  loss: 9.333635, mae: 164.794791, mean_q: 224.021219, mean_eps: 0.875804
 14400/100000: episode: 36, duration: 33.262s, episode steps: 400, steps per second:  12, episode reward: 3137.181, mean reward:  7.843 [ 0.000, 31.451], mean action: 1.587 [0.000, 3.000],  loss: 8.744994, mae: 169.870384, mean_q: 231.077661, mean_eps: 0.872205
 14800/100000: episode: 37, duration: 33.281s, episode steps: 400, steps per second:  12, episode reward: 1661.288, mean reward:  4.153 [ 0.000, 20.376], mean action: 1.597 [0.000, 3.000],  loss: 8.648882, mae: 170.605441, mean_q: 230.865293, mean_eps: 0.868604
 15200/100000: episode: 38, duration: 33.196s, episode steps: 400, steps per second:  12, episode reward: 2210.930, mean reward:  5.527 [ 0.000, 23.286], mean action: 1.560 [0.000, 3.000],  loss: 8.858080, mae: 173.821810, mean_q: 234.713836, mean_eps: 0.865004
 15600/100000: episode: 39, duration: 33.110s, episode steps: 400, steps per second:  12, episode reward: 1853.636, mean reward:  4.634 [ 0.000, 23.910], mean action: 1.580 [0.000, 3.000],  loss: 11.181506, mae: 177.441640, mean_q: 239.335777, mean_eps: 0.861405
 16000/100000: episode: 40, duration: 33.138s, episode steps: 400, steps per second:  12, episode reward: 2139.963, mean reward:  5.350 [ 0.000, 25.168], mean action: 1.673 [0.000, 3.000],  loss: 9.025004, mae: 178.241857, mean_q: 241.316518, mean_eps: 0.857804
 16400/100000: episode: 41, duration: 32.955s, episode steps: 400, steps per second:  12, episode reward: 2201.929, mean reward:  5.505 [ 0.000, 30.489], mean action: 1.605 [0.000, 3.000],  loss: 8.128712, mae: 182.898754, mean_q: 247.065866, mean_eps: 0.854204
 16800/100000: episode: 42, duration: 33.063s, episode steps: 400, steps per second:  12, episode reward: 2700.718, mean reward:  6.752 [ 0.000, 31.119], mean action: 1.433 [0.000, 3.000],  loss: 12.004596, mae: 182.993011, mean_q: 248.514454, mean_eps: 0.850604
 17200/100000: episode: 43, duration: 33.320s, episode steps: 400, steps per second:  12, episode reward: 1689.907, mean reward:  4.225 [ 0.000, 18.832], mean action: 1.580 [0.000, 3.000],  loss: 9.705067, mae: 186.199732, mean_q: 252.233162, mean_eps: 0.847004
 17600/100000: episode: 44, duration: 33.098s, episode steps: 400, steps per second:  12, episode reward: 2808.479, mean reward:  7.021 [ 0.000, 31.306], mean action: 1.510 [0.000, 3.000],  loss: 10.558945, mae: 189.946483, mean_q: 257.148386, mean_eps: 0.843405
 18000/100000: episode: 45, duration: 33.360s, episode steps: 400, steps per second:  12, episode reward: 1515.978, mean reward:  3.790 [ 0.000, 20.920], mean action: 1.532 [0.000, 3.000],  loss: 10.087710, mae: 189.957565, mean_q: 257.198377, mean_eps: 0.839805
 18400/100000: episode: 46, duration: 33.168s, episode steps: 400, steps per second:  12, episode reward: 4214.910, mean reward: 10.537 [ 0.000, 36.685], mean action: 1.387 [0.000, 3.000],  loss: 10.612134, mae: 195.779445, mean_q: 263.351132, mean_eps: 0.836205
 18800/100000: episode: 47, duration: 33.202s, episode steps: 400, steps per second:  12, episode reward: 2771.092, mean reward:  6.928 [ 0.000, 30.573], mean action: 1.445 [0.000, 3.000],  loss: 8.848546, mae: 195.133325, mean_q: 264.224340, mean_eps: 0.832604
 19200/100000: episode: 48, duration: 33.286s, episode steps: 400, steps per second:  12, episode reward: 1947.076, mean reward:  4.868 [ 0.000, 25.921], mean action: 1.455 [0.000, 3.000],  loss: 9.368615, mae: 197.736821, mean_q: 266.837984, mean_eps: 0.829005
 19600/100000: episode: 49, duration: 33.060s, episode steps: 400, steps per second:  12, episode reward: 2319.783, mean reward:  5.799 [ 0.000, 27.944], mean action: 1.400 [0.000, 3.000],  loss: 10.755608, mae: 200.825931, mean_q: 270.761971, mean_eps: 0.825404
 20000/100000: episode: 50, duration: 33.244s, episode steps: 400, steps per second:  12, episode reward: 2219.190, mean reward:  5.548 [ 0.000, 29.641], mean action: 1.482 [0.000, 3.000],  loss: 8.690859, mae: 201.257181, mean_q: 271.613593, mean_eps: 0.821805
 20400/100000: episode: 51, duration: 33.329s, episode steps: 400, steps per second:  12, episode reward: 1609.094, mean reward:  4.023 [ 0.000, 16.858], mean action: 1.595 [0.000, 3.000],  loss: 8.165656, mae: 206.801732, mean_q: 279.030536, mean_eps: 0.818204
 20800/100000: episode: 52, duration: 33.323s, episode steps: 400, steps per second:  12, episode reward: 3766.209, mean reward:  9.416 [ 0.000, 42.435], mean action: 1.525 [0.000, 3.000],  loss: 8.333393, mae: 206.717807, mean_q: 279.210598, mean_eps: 0.814604
 21200/100000: episode: 53, duration: 33.406s, episode steps: 400, steps per second:  12, episode reward: 2359.947, mean reward:  5.900 [ 0.000, 35.009], mean action: 1.577 [0.000, 3.000],  loss: 10.805314, mae: 209.311842, mean_q: 282.454887, mean_eps: 0.811004
 21600/100000: episode: 54, duration: 33.440s, episode steps: 400, steps per second:  12, episode reward: 2802.768, mean reward:  7.007 [ 0.000, 32.466], mean action: 1.567 [0.000, 3.000],  loss: 8.096558, mae: 211.048791, mean_q: 286.108821, mean_eps: 0.807404
 22000/100000: episode: 55, duration: 33.329s, episode steps: 400, steps per second:  12, episode reward: 2365.115, mean reward:  5.913 [ 0.000, 33.587], mean action: 1.508 [0.000, 3.000],  loss: 10.232690, mae: 210.444719, mean_q: 283.329435, mean_eps: 0.803804
 22400/100000: episode: 56, duration: 33.449s, episode steps: 400, steps per second:  12, episode reward: 2212.590, mean reward:  5.531 [ 0.000, 29.345], mean action: 1.532 [0.000, 3.000],  loss: 10.049865, mae: 218.428800, mean_q: 294.419214, mean_eps: 0.800204
 22800/100000: episode: 57, duration: 33.139s, episode steps: 400, steps per second:  12, episode reward: 5047.093, mean reward: 12.618 [ 0.000, 37.022], mean action: 1.442 [0.000, 3.000],  loss: 11.773152, mae: 220.013857, mean_q: 296.227440, mean_eps: 0.796604
 23200/100000: episode: 58, duration: 33.568s, episode steps: 400, steps per second:  12, episode reward: 3576.120, mean reward:  8.940 [ 0.000, 44.276], mean action: 1.535 [0.000, 3.000],  loss: 9.736318, mae: 220.325303, mean_q: 297.306694, mean_eps: 0.793005
 23600/100000: episode: 59, duration: 33.101s, episode steps: 400, steps per second:  12, episode reward: 2771.021, mean reward:  6.928 [ 0.000, 41.518], mean action: 1.522 [0.000, 3.000],  loss: 7.826050, mae: 225.141300, mean_q: 304.261092, mean_eps: 0.789404
 24000/100000: episode: 60, duration: 33.146s, episode steps: 400, steps per second:  12, episode reward: 2749.119, mean reward:  6.873 [ 0.000, 29.487], mean action: 1.562 [0.000, 3.000],  loss: 8.696684, mae: 225.856839, mean_q: 305.269661, mean_eps: 0.785805
 24400/100000: episode: 61, duration: 33.412s, episode steps: 400, steps per second:  12, episode reward: 3048.563, mean reward:  7.621 [ 0.000, 29.063], mean action: 1.468 [0.000, 3.000],  loss: 10.335258, mae: 229.953482, mean_q: 310.170444, mean_eps: 0.782204
 24800/100000: episode: 62, duration: 33.343s, episode steps: 400, steps per second:  12, episode reward: 4128.121, mean reward: 10.320 [ 0.000, 40.002], mean action: 1.500 [0.000, 3.000],  loss: 9.981004, mae: 230.304322, mean_q: 310.670203, mean_eps: 0.778605
 25200/100000: episode: 63, duration: 33.734s, episode steps: 400, steps per second:  12, episode reward: 2651.659, mean reward:  6.629 [ 0.000, 28.669], mean action: 1.458 [0.000, 3.000],  loss: 8.586317, mae: 230.715360, mean_q: 310.039076, mean_eps: 0.775004
 25600/100000: episode: 64, duration: 33.417s, episode steps: 400, steps per second:  12, episode reward: 3003.944, mean reward:  7.510 [ 0.000, 31.897], mean action: 1.590 [0.000, 3.000],  loss: 11.715191, mae: 229.931893, mean_q: 308.662601, mean_eps: 0.771405
 26000/100000: episode: 65, duration: 33.391s, episode steps: 400, steps per second:  12, episode reward: 1970.981, mean reward:  4.927 [ 0.000, 25.540], mean action: 1.607 [0.000, 3.000],  loss: 8.821117, mae: 231.291367, mean_q: 311.828508, mean_eps: 0.767805
 26400/100000: episode: 66, duration: 33.487s, episode steps: 400, steps per second:  12, episode reward: 3785.354, mean reward:  9.463 [ 0.000, 30.831], mean action: 1.558 [0.000, 3.000],  loss: 8.683697, mae: 237.121642, mean_q: 319.466909, mean_eps: 0.764205
 26800/100000: episode: 67, duration: 33.324s, episode steps: 400, steps per second:  12, episode reward: 2147.162, mean reward:  5.368 [ 0.000, 22.917], mean action: 1.595 [0.000, 3.000],  loss: 9.349097, mae: 236.032380, mean_q: 318.057208, mean_eps: 0.760605
 27200/100000: episode: 68, duration: 33.404s, episode steps: 400, steps per second:  12, episode reward: 2886.738, mean reward:  7.217 [ 0.000, 26.910], mean action: 1.550 [0.000, 3.000],  loss: 10.837138, mae: 237.919828, mean_q: 320.962782, mean_eps: 0.757004
 27600/100000: episode: 69, duration: 33.329s, episode steps: 400, steps per second:  12, episode reward: 5000.306, mean reward: 12.501 [ 0.000, 67.921], mean action: 1.327 [0.000, 3.000],  loss: 8.375032, mae: 239.635271, mean_q: 322.829945, mean_eps: 0.753405
 28000/100000: episode: 70, duration: 33.218s, episode steps: 400, steps per second:  12, episode reward: 3475.920, mean reward:  8.690 [ 0.000, 43.931], mean action: 1.488 [0.000, 3.000],  loss: 8.821926, mae: 239.965394, mean_q: 322.923999, mean_eps: 0.749804
 28400/100000: episode: 71, duration: 33.204s, episode steps: 400, steps per second:  12, episode reward: 3128.592, mean reward:  7.821 [ 0.000, 37.332], mean action: 1.512 [0.000, 3.000],  loss: 8.981706, mae: 243.087314, mean_q: 327.051032, mean_eps: 0.746205
 28800/100000: episode: 72, duration: 33.320s, episode steps: 400, steps per second:  12, episode reward: 3007.410, mean reward:  7.519 [ 0.000, 44.041], mean action: 1.802 [0.000, 3.000],  loss: 9.306456, mae: 245.435369, mean_q: 330.878008, mean_eps: 0.742604
 29200/100000: episode: 73, duration: 33.548s, episode steps: 400, steps per second:  12, episode reward: 2452.605, mean reward:  6.132 [ 0.000, 26.195], mean action: 1.520 [0.000, 3.000],  loss: 10.360539, mae: 246.296958, mean_q: 331.347516, mean_eps: 0.739005
 29600/100000: episode: 74, duration: 33.654s, episode steps: 400, steps per second:  12, episode reward: 1874.053, mean reward:  4.685 [ 0.000, 25.961], mean action: 1.595 [0.000, 3.000],  loss: 10.991996, mae: 248.757304, mean_q: 334.865186, mean_eps: 0.735404
 30000/100000: episode: 75, duration: 33.439s, episode steps: 400, steps per second:  12, episode reward: 2397.705, mean reward:  5.994 [ 0.000, 24.814], mean action: 1.552 [0.000, 3.000],  loss: 8.635754, mae: 248.300017, mean_q: 334.646196, mean_eps: 0.731805
 30400/100000: episode: 76, duration: 33.405s, episode steps: 400, steps per second:  12, episode reward: 3214.452, mean reward:  8.036 [ 0.000, 37.436], mean action: 1.520 [0.000, 3.000],  loss: 10.619405, mae: 253.362860, mean_q: 338.996640, mean_eps: 0.728204
 30800/100000: episode: 77, duration: 33.356s, episode steps: 400, steps per second:  12, episode reward: 2944.653, mean reward:  7.362 [ 0.000, 39.827], mean action: 1.540 [0.000, 3.000],  loss: 9.075930, mae: 253.358151, mean_q: 340.934043, mean_eps: 0.724605
 31200/100000: episode: 78, duration: 33.402s, episode steps: 400, steps per second:  12, episode reward: 3953.299, mean reward:  9.883 [ 0.000, 55.696], mean action: 1.560 [0.000, 3.000],  loss: 8.791047, mae: 256.469930, mean_q: 345.083067, mean_eps: 0.721004
 31600/100000: episode: 79, duration: 33.414s, episode steps: 400, steps per second:  12, episode reward: 4016.164, mean reward: 10.040 [ 0.000, 43.679], mean action: 1.630 [0.000, 3.000],  loss: 10.037943, mae: 259.342503, mean_q: 347.818527, mean_eps: 0.717405
 32000/100000: episode: 80, duration: 33.117s, episode steps: 400, steps per second:  12, episode reward: 4287.026, mean reward: 10.718 [ 0.000, 54.048], mean action: 1.600 [0.000, 3.000],  loss: 9.873108, mae: 260.211124, mean_q: 349.928456, mean_eps: 0.713804
 32400/100000: episode: 81, duration: 33.379s, episode steps: 400, steps per second:  12, episode reward: 2552.020, mean reward:  6.380 [ 0.000, 32.405], mean action: 1.692 [0.000, 3.000],  loss: 9.352043, mae: 263.504342, mean_q: 354.649603, mean_eps: 0.710205
 32800/100000: episode: 82, duration: 32.999s, episode steps: 400, steps per second:  12, episode reward: 7169.075, mean reward: 17.923 [ 0.000, 59.205], mean action: 1.460 [0.000, 3.000],  loss: 8.559668, mae: 263.568671, mean_q: 353.997200, mean_eps: 0.706604
 33200/100000: episode: 83, duration: 33.431s, episode steps: 400, steps per second:  12, episode reward: 5652.559, mean reward: 14.131 [ 0.000, 51.163], mean action: 1.308 [0.000, 3.000],  loss: 8.816767, mae: 266.055533, mean_q: 357.465067, mean_eps: 0.703005
 33600/100000: episode: 84, duration: 33.643s, episode steps: 400, steps per second:  12, episode reward: 2909.301, mean reward:  7.273 [ 0.000, 43.361], mean action: 1.357 [0.000, 3.000],  loss: 8.871235, mae: 266.481786, mean_q: 358.048370, mean_eps: 0.699404
 34000/100000: episode: 85, duration: 33.341s, episode steps: 400, steps per second:  12, episode reward: 3120.996, mean reward:  7.802 [ 0.000, 36.512], mean action: 1.625 [0.000, 3.000],  loss: 10.250192, mae: 266.699981, mean_q: 358.410857, mean_eps: 0.695804
 34400/100000: episode: 86, duration: 33.265s, episode steps: 400, steps per second:  12, episode reward: 5521.137, mean reward: 13.803 [ 0.000, 41.762], mean action: 1.415 [0.000, 3.000],  loss: 10.933522, mae: 269.658018, mean_q: 362.151557, mean_eps: 0.692205
 34800/100000: episode: 87, duration: 33.468s, episode steps: 400, steps per second:  12, episode reward: 2727.303, mean reward:  6.818 [ 0.000, 25.152], mean action: 1.445 [0.000, 3.000],  loss: 8.765546, mae: 270.755943, mean_q: 363.887793, mean_eps: 0.688604
 35200/100000: episode: 88, duration: 32.721s, episode steps: 400, steps per second:  12, episode reward: 6542.759, mean reward: 16.357 [ 0.000, 45.495], mean action: 1.180 [0.000, 3.000],  loss: 8.976474, mae: 270.861824, mean_q: 363.736989, mean_eps: 0.685005
 35600/100000: episode: 89, duration: 33.589s, episode steps: 400, steps per second:  12, episode reward: 1682.815, mean reward:  4.207 [ 0.000, 21.455], mean action: 1.705 [0.000, 3.000],  loss: 11.434491, mae: 273.286432, mean_q: 367.404335, mean_eps: 0.681404
 36000/100000: episode: 90, duration: 33.113s, episode steps: 400, steps per second:  12, episode reward: 3633.764, mean reward:  9.084 [ 0.000, 32.478], mean action: 1.625 [0.000, 3.000],  loss: 10.139497, mae: 275.605215, mean_q: 370.870740, mean_eps: 0.677805
 36400/100000: episode: 91, duration: 32.947s, episode steps: 400, steps per second:  12, episode reward: 4213.965, mean reward: 10.535 [ 0.000, 48.087], mean action: 1.423 [0.000, 3.000],  loss: 10.614223, mae: 280.805493, mean_q: 376.364032, mean_eps: 0.674204
 36800/100000: episode: 92, duration: 33.491s, episode steps: 400, steps per second:  12, episode reward: 4691.330, mean reward: 11.728 [ 0.000, 47.914], mean action: 1.545 [0.000, 3.000],  loss: 11.608240, mae: 283.305564, mean_q: 379.760492, mean_eps: 0.670605
 37200/100000: episode: 93, duration: 33.242s, episode steps: 400, steps per second:  12, episode reward: 4924.314, mean reward: 12.311 [ 0.000, 75.817], mean action: 1.532 [0.000, 3.000],  loss: 12.223863, mae: 285.625672, mean_q: 383.481102, mean_eps: 0.667004
 37600/100000: episode: 94, duration: 33.224s, episode steps: 400, steps per second:  12, episode reward: 1805.090, mean reward:  4.513 [ 0.000, 24.167], mean action: 1.562 [0.000, 3.000],  loss: 11.238788, mae: 288.560296, mean_q: 387.122589, mean_eps: 0.663405
 38000/100000: episode: 95, duration: 33.197s, episode steps: 400, steps per second:  12, episode reward: 2343.239, mean reward:  5.858 [ 0.000, 25.247], mean action: 1.460 [0.000, 3.000],  loss: 11.231331, mae: 288.003337, mean_q: 386.696340, mean_eps: 0.659804
 38400/100000: episode: 96, duration: 33.399s, episode steps: 400, steps per second:  12, episode reward: 2512.173, mean reward:  6.280 [ 0.000, 27.335], mean action: 1.515 [0.000, 3.000],  loss: 10.989482, mae: 292.493448, mean_q: 392.231269, mean_eps: 0.656205
 38800/100000: episode: 97, duration: 33.656s, episode steps: 400, steps per second:  12, episode reward: 2857.142, mean reward:  7.143 [ 0.000, 27.120], mean action: 1.552 [0.000, 3.000],  loss: 9.755356, mae: 293.380647, mean_q: 393.725551, mean_eps: 0.652604
 39200/100000: episode: 98, duration: 33.277s, episode steps: 400, steps per second:  12, episode reward: 3425.591, mean reward:  8.564 [ 0.000, 43.452], mean action: 1.445 [0.000, 3.000],  loss: 12.682854, mae: 292.754799, mean_q: 391.492904, mean_eps: 0.649005
 39600/100000: episode: 99, duration: 33.579s, episode steps: 400, steps per second:  12, episode reward: 3864.107, mean reward:  9.660 [ 0.000, 77.713], mean action: 1.528 [0.000, 3.000],  loss: 12.589670, mae: 297.669528, mean_q: 398.093398, mean_eps: 0.645404
 40000/100000: episode: 100, duration: 32.914s, episode steps: 400, steps per second:  12, episode reward: 9241.429, mean reward: 23.104 [ 0.000, 69.588], mean action: 1.095 [0.000, 3.000],  loss: 7.875246, mae: 295.902946, mean_q: 397.437861, mean_eps: 0.641805
 40400/100000: episode: 101, duration: 33.255s, episode steps: 400, steps per second:  12, episode reward: 7918.781, mean reward: 19.797 [ 0.000, 52.022], mean action: 1.387 [0.000, 3.000],  loss: 9.031803, mae: 301.806438, mean_q: 404.335335, mean_eps: 0.638205
 40800/100000: episode: 102, duration: 32.916s, episode steps: 400, steps per second:  12, episode reward: 3843.601, mean reward:  9.609 [ 0.000, 59.430], mean action: 1.235 [0.000, 3.000],  loss: 10.548713, mae: 301.933492, mean_q: 404.872609, mean_eps: 0.634604
 41200/100000: episode: 103, duration: 33.896s, episode steps: 400, steps per second:  12, episode reward: 4580.009, mean reward: 11.450 [ 0.000, 38.528], mean action: 1.220 [0.000, 3.000],  loss: 9.357488, mae: 303.226209, mean_q: 406.575154, mean_eps: 0.631004
 41600/100000: episode: 104, duration: 33.062s, episode steps: 400, steps per second:  12, episode reward: 1527.911, mean reward:  3.820 [ 0.000, 17.566], mean action: 1.558 [0.000, 3.000],  loss: 9.681143, mae: 305.108640, mean_q: 409.319533, mean_eps: 0.627404
 42000/100000: episode: 105, duration: 33.460s, episode steps: 400, steps per second:  12, episode reward: 3272.519, mean reward:  8.181 [ 0.000, 37.245], mean action: 1.555 [0.000, 3.000],  loss: 10.593146, mae: 306.257271, mean_q: 411.679786, mean_eps: 0.623804
 42400/100000: episode: 106, duration: 32.896s, episode steps: 400, steps per second:  12, episode reward: 4045.307, mean reward: 10.113 [ 0.000, 52.400], mean action: 1.542 [0.000, 3.000],  loss: 10.832457, mae: 309.450716, mean_q: 415.670975, mean_eps: 0.620204
 42800/100000: episode: 107, duration: 33.014s, episode steps: 400, steps per second:  12, episode reward: 3183.755, mean reward:  7.959 [ 0.000, 39.372], mean action: 1.550 [0.000, 3.000],  loss: 11.441731, mae: 310.908868, mean_q: 417.094363, mean_eps: 0.616604
 43200/100000: episode: 108, duration: 33.267s, episode steps: 400, steps per second:  12, episode reward: 1641.937, mean reward:  4.105 [ 0.000, 18.129], mean action: 1.715 [0.000, 3.000],  loss: 11.556863, mae: 314.426840, mean_q: 421.783844, mean_eps: 0.613004
 43600/100000: episode: 109, duration: 33.142s, episode steps: 400, steps per second:  12, episode reward: 1476.783, mean reward:  3.692 [ 0.000, 21.260], mean action: 1.597 [0.000, 3.000],  loss: 10.677310, mae: 314.930544, mean_q: 422.984792, mean_eps: 0.609405
 44000/100000: episode: 110, duration: 33.202s, episode steps: 400, steps per second:  12, episode reward: 9054.021, mean reward: 22.635 [ 0.000, 88.129], mean action: 1.345 [0.000, 3.000],  loss: 12.122204, mae: 314.768447, mean_q: 421.644054, mean_eps: 0.605804
 44400/100000: episode: 111, duration: 33.185s, episode steps: 400, steps per second:  12, episode reward: 3301.203, mean reward:  8.253 [ 0.000, 29.671], mean action: 1.575 [0.000, 3.000],  loss: 9.972826, mae: 320.509347, mean_q: 430.262401, mean_eps: 0.602205
 44800/100000: episode: 112, duration: 33.124s, episode steps: 400, steps per second:  12, episode reward: 3395.781, mean reward:  8.489 [ 0.000, 36.575], mean action: 1.558 [0.000, 3.000],  loss: 10.327336, mae: 321.526154, mean_q: 430.763618, mean_eps: 0.598604
 45200/100000: episode: 113, duration: 33.165s, episode steps: 400, steps per second:  12, episode reward: 4528.375, mean reward: 11.321 [ 0.000, 63.438], mean action: 1.590 [0.000, 3.000],  loss: 11.490174, mae: 321.702988, mean_q: 431.229669, mean_eps: 0.595005
 45600/100000: episode: 114, duration: 32.893s, episode steps: 400, steps per second:  12, episode reward: 6313.210, mean reward: 15.783 [ 0.000, 77.373], mean action: 1.440 [0.000, 3.000],  loss: 10.025077, mae: 324.922230, mean_q: 435.530286, mean_eps: 0.591404
 46000/100000: episode: 115, duration: 33.143s, episode steps: 400, steps per second:  12, episode reward: 4611.208, mean reward: 11.528 [ 0.000, 43.753], mean action: 1.452 [0.000, 3.000],  loss: 12.382067, mae: 323.512427, mean_q: 432.714739, mean_eps: 0.587804
 46400/100000: episode: 116, duration: 32.714s, episode steps: 400, steps per second:  12, episode reward: 3261.863, mean reward:  8.155 [ 0.000, 51.238], mean action: 1.812 [0.000, 3.000],  loss: 10.512950, mae: 325.038172, mean_q: 435.119707, mean_eps: 0.584205
 46800/100000: episode: 117, duration: 33.297s, episode steps: 400, steps per second:  12, episode reward: 7828.923, mean reward: 19.572 [ 0.000, 77.563], mean action: 1.510 [0.000, 3.000],  loss: 11.693765, mae: 325.436081, mean_q: 436.678142, mean_eps: 0.580605
 47200/100000: episode: 118, duration: 33.465s, episode steps: 400, steps per second:  12, episode reward: 4576.495, mean reward: 11.441 [ 0.000, 42.753], mean action: 1.498 [0.000, 3.000],  loss: 9.642576, mae: 327.952780, mean_q: 440.199867, mean_eps: 0.577004
 47600/100000: episode: 119, duration: 33.087s, episode steps: 400, steps per second:  12, episode reward: 6332.034, mean reward: 15.830 [ 0.000, 64.545], mean action: 1.375 [0.000, 3.000],  loss: 11.110588, mae: 331.642837, mean_q: 445.087619, mean_eps: 0.573405
 48000/100000: episode: 120, duration: 33.353s, episode steps: 400, steps per second:  12, episode reward: 4089.297, mean reward: 10.223 [ 0.000, 42.796], mean action: 1.482 [0.000, 3.000],  loss: 13.145089, mae: 332.103809, mean_q: 444.750151, mean_eps: 0.569805
 48400/100000: episode: 121, duration: 33.509s, episode steps: 400, steps per second:  12, episode reward: 4765.492, mean reward: 11.914 [ 0.000, 52.985], mean action: 1.482 [0.000, 3.000],  loss: 12.011339, mae: 335.475712, mean_q: 449.697836, mean_eps: 0.566204
 48800/100000: episode: 122, duration: 33.189s, episode steps: 400, steps per second:  12, episode reward: 4245.089, mean reward: 10.613 [ 0.000, 61.924], mean action: 1.552 [0.000, 3.000],  loss: 13.913396, mae: 333.605858, mean_q: 445.933076, mean_eps: 0.562604
 49200/100000: episode: 123, duration: 33.245s, episode steps: 400, steps per second:  12, episode reward: 5598.083, mean reward: 13.995 [ 0.000, 56.069], mean action: 1.365 [0.000, 3.000],  loss: 12.292122, mae: 333.708344, mean_q: 447.791485, mean_eps: 0.559005
 49600/100000: episode: 124, duration: 33.503s, episode steps: 400, steps per second:  12, episode reward: 2612.880, mean reward:  6.532 [ 0.000, 45.733], mean action: 1.710 [0.000, 3.000],  loss: 11.890198, mae: 336.382209, mean_q: 451.556802, mean_eps: 0.555404
 50000/100000: episode: 125, duration: 33.180s, episode steps: 400, steps per second:  12, episode reward: 2832.811, mean reward:  7.082 [ 0.000, 43.113], mean action: 1.645 [0.000, 3.000],  loss: 12.724019, mae: 338.911598, mean_q: 454.169341, mean_eps: 0.551804
 50400/100000: episode: 126, duration: 32.845s, episode steps: 400, steps per second:  12, episode reward: 2329.557, mean reward:  5.824 [ 0.000, 49.292], mean action: 1.597 [0.000, 3.000],  loss: 11.222419, mae: 343.739252, mean_q: 460.898765, mean_eps: 0.548204
 50800/100000: episode: 127, duration: 33.242s, episode steps: 400, steps per second:  12, episode reward: 4282.552, mean reward: 10.706 [ 0.000, 37.497], mean action: 1.558 [0.000, 3.000],  loss: 12.658752, mae: 343.430250, mean_q: 459.879368, mean_eps: 0.544605
 51200/100000: episode: 128, duration: 33.161s, episode steps: 400, steps per second:  12, episode reward: 7153.586, mean reward: 17.884 [ 0.000, 71.760], mean action: 1.285 [0.000, 3.000],  loss: 17.231674, mae: 341.532543, mean_q: 456.085269, mean_eps: 0.541004
 51600/100000: episode: 129, duration: 32.924s, episode steps: 400, steps per second:  12, episode reward: 8119.706, mean reward: 20.299 [ 0.000, 56.442], mean action: 1.258 [0.000, 3.000],  loss: 12.085723, mae: 346.101787, mean_q: 464.425896, mean_eps: 0.537404
 52000/100000: episode: 130, duration: 33.417s, episode steps: 400, steps per second:  12, episode reward: 1427.373, mean reward:  3.568 [ 0.000, 27.242], mean action: 1.645 [0.000, 3.000],  loss: 14.643584, mae: 346.691182, mean_q: 464.427592, mean_eps: 0.533805
 52400/100000: episode: 131, duration: 33.007s, episode steps: 400, steps per second:  12, episode reward: 4221.467, mean reward: 10.554 [ 0.000, 53.003], mean action: 1.603 [0.000, 3.000],  loss: 15.807428, mae: 350.491737, mean_q: 467.640347, mean_eps: 0.530204
 52800/100000: episode: 132, duration: 33.102s, episode steps: 400, steps per second:  12, episode reward: 4902.091, mean reward: 12.255 [ 0.000, 47.360], mean action: 1.438 [0.000, 3.000],  loss: 13.179578, mae: 354.487798, mean_q: 474.739236, mean_eps: 0.526604
 53200/100000: episode: 133, duration: 32.807s, episode steps: 400, steps per second:  12, episode reward: 4847.490, mean reward: 12.119 [ 0.000, 51.445], mean action: 1.270 [0.000, 3.000],  loss: 13.825329, mae: 354.347717, mean_q: 474.441888, mean_eps: 0.523004
 53600/100000: episode: 134, duration: 33.166s, episode steps: 400, steps per second:  12, episode reward: 3863.017, mean reward:  9.658 [ 0.000, 41.286], mean action: 1.465 [0.000, 3.000],  loss: 13.263242, mae: 359.640202, mean_q: 481.792109, mean_eps: 0.519404
 54000/100000: episode: 135, duration: 33.041s, episode steps: 400, steps per second:  12, episode reward: 8886.114, mean reward: 22.215 [ 0.000, 75.806], mean action: 1.260 [0.000, 3.000],  loss: 11.913911, mae: 357.692552, mean_q: 479.264548, mean_eps: 0.515804
 54400/100000: episode: 136, duration: 32.991s, episode steps: 400, steps per second:  12, episode reward: 6612.278, mean reward: 16.531 [ 0.000, 55.850], mean action: 1.323 [0.000, 3.000],  loss: 12.519387, mae: 364.873561, mean_q: 487.790692, mean_eps: 0.512204
 54800/100000: episode: 137, duration: 32.960s, episode steps: 400, steps per second:  12, episode reward: 2190.573, mean reward:  5.476 [ 0.000, 41.529], mean action: 2.025 [0.000, 3.000],  loss: 10.730222, mae: 365.221498, mean_q: 489.210785, mean_eps: 0.508605
 55200/100000: episode: 138, duration: 33.077s, episode steps: 400, steps per second:  12, episode reward: 5925.119, mean reward: 14.813 [ 0.000, 56.237], mean action: 1.353 [0.000, 3.000],  loss: 13.622364, mae: 367.164710, mean_q: 491.703522, mean_eps: 0.505004
 55600/100000: episode: 139, duration: 32.823s, episode steps: 400, steps per second:  12, episode reward: 5223.364, mean reward: 13.058 [ 0.000, 53.725], mean action: 1.413 [0.000, 3.000],  loss: 14.030873, mae: 370.437038, mean_q: 496.269503, mean_eps: 0.501404
 56000/100000: episode: 140, duration: 33.050s, episode steps: 400, steps per second:  12, episode reward: 8264.582, mean reward: 20.661 [ 0.000, 75.172], mean action: 1.323 [0.000, 3.000],  loss: 12.497675, mae: 372.750040, mean_q: 501.303424, mean_eps: 0.497805
 56400/100000: episode: 141, duration: 32.944s, episode steps: 400, steps per second:  12, episode reward: 1994.341, mean reward:  4.986 [ 0.000, 37.113], mean action: 1.735 [0.000, 3.000],  loss: 11.724162, mae: 378.776230, mean_q: 508.897894, mean_eps: 0.494205
 56800/100000: episode: 142, duration: 32.863s, episode steps: 400, steps per second:  12, episode reward: 3682.131, mean reward:  9.205 [ 0.000, 59.087], mean action: 1.433 [0.000, 3.000],  loss: 13.858703, mae: 379.773541, mean_q: 509.449539, mean_eps: 0.490605
 57200/100000: episode: 143, duration: 32.877s, episode steps: 400, steps per second:  12, episode reward: 11008.812, mean reward: 27.522 [ 0.000, 132.749], mean action: 1.242 [0.000, 3.000],  loss: 12.609987, mae: 378.460482, mean_q: 507.131442, mean_eps: 0.487005
 57600/100000: episode: 144, duration: 32.701s, episode steps: 400, steps per second:  12, episode reward: 3856.107, mean reward:  9.640 [ 0.000, 47.849], mean action: 1.188 [0.000, 3.000],  loss: 11.934850, mae: 381.506617, mean_q: 511.128889, mean_eps: 0.483405
 58000/100000: episode: 145, duration: 33.455s, episode steps: 400, steps per second:  12, episode reward: 5493.151, mean reward: 13.733 [ 0.000, 68.072], mean action: 1.340 [0.000, 3.000],  loss: 12.645977, mae: 382.556195, mean_q: 512.101565, mean_eps: 0.479805
 58400/100000: episode: 146, duration: 33.133s, episode steps: 400, steps per second:  12, episode reward: 6537.303, mean reward: 16.343 [ 0.000, 70.285], mean action: 1.413 [0.000, 3.000],  loss: 12.436395, mae: 383.756105, mean_q: 513.639285, mean_eps: 0.476204
 58800/100000: episode: 147, duration: 33.232s, episode steps: 400, steps per second:  12, episode reward: 5505.015, mean reward: 13.763 [ 0.000, 55.795], mean action: 1.515 [0.000, 3.000],  loss: 11.554948, mae: 384.569725, mean_q: 515.640815, mean_eps: 0.472604
 59200/100000: episode: 148, duration: 33.319s, episode steps: 400, steps per second:  12, episode reward: 9745.298, mean reward: 24.363 [ 0.000, 88.692], mean action: 1.323 [0.000, 3.000],  loss: 16.869041, mae: 388.906932, mean_q: 521.318863, mean_eps: 0.469004
 59600/100000: episode: 149, duration: 33.354s, episode steps: 400, steps per second:  12, episode reward: 2558.877, mean reward:  6.397 [ 0.000, 31.714], mean action: 1.410 [0.000, 3.000],  loss: 13.388186, mae: 389.939246, mean_q: 523.146362, mean_eps: 0.465404
 60000/100000: episode: 150, duration: 33.435s, episode steps: 400, steps per second:  12, episode reward: 3967.733, mean reward:  9.919 [ 0.000, 79.081], mean action: 1.677 [0.000, 3.000],  loss: 17.387057, mae: 390.432713, mean_q: 522.925804, mean_eps: 0.461804
 60400/100000: episode: 151, duration: 32.981s, episode steps: 400, steps per second:  12, episode reward: 7669.366, mean reward: 19.173 [ 0.000, 94.340], mean action: 1.375 [0.000, 3.000],  loss: 15.433857, mae: 393.264301, mean_q: 525.839965, mean_eps: 0.458204
 60800/100000: episode: 152, duration: 33.341s, episode steps: 400, steps per second:  12, episode reward: 3277.067, mean reward:  8.193 [ 0.000, 57.674], mean action: 1.548 [0.000, 3.000],  loss: 17.799801, mae: 398.238421, mean_q: 532.776922, mean_eps: 0.454604
 61200/100000: episode: 153, duration: 33.101s, episode steps: 400, steps per second:  12, episode reward: 11153.855, mean reward: 27.885 [ 0.000, 76.493], mean action: 1.105 [0.000, 3.000],  loss: 13.982296, mae: 402.912107, mean_q: 540.319466, mean_eps: 0.451004
 61600/100000: episode: 154, duration: 33.051s, episode steps: 400, steps per second:  12, episode reward: 6243.132, mean reward: 15.608 [ 0.000, 71.650], mean action: 1.617 [0.000, 3.000],  loss: 16.815429, mae: 406.547918, mean_q: 543.010416, mean_eps: 0.447404
 62000/100000: episode: 155, duration: 33.197s, episode steps: 400, steps per second:  12, episode reward: 9110.032, mean reward: 22.775 [ 0.000, 101.747], mean action: 1.373 [0.000, 3.000],  loss: 13.605443, mae: 405.622384, mean_q: 543.617501, mean_eps: 0.443804
 62400/100000: episode: 156, duration: 32.947s, episode steps: 400, steps per second:  12, episode reward: 2691.120, mean reward:  6.728 [ 0.000, 50.579], mean action: 1.688 [0.000, 3.000],  loss: 14.377943, mae: 408.197576, mean_q: 546.588050, mean_eps: 0.440204
 62800/100000: episode: 157, duration: 33.291s, episode steps: 400, steps per second:  12, episode reward: 3177.964, mean reward:  7.945 [ 0.000, 34.250], mean action: 1.653 [0.000, 3.000],  loss: 14.757786, mae: 407.126536, mean_q: 545.951324, mean_eps: 0.436604
 63200/100000: episode: 158, duration: 32.940s, episode steps: 400, steps per second:  12, episode reward: 7710.241, mean reward: 19.276 [ 0.000, 65.685], mean action: 1.185 [0.000, 3.000],  loss: 15.740326, mae: 409.346943, mean_q: 547.813171, mean_eps: 0.433004
 63600/100000: episode: 159, duration: 32.811s, episode steps: 400, steps per second:  12, episode reward: 11406.906, mean reward: 28.517 [ 0.000, 90.293], mean action: 1.188 [0.000, 3.000],  loss: 12.370958, mae: 412.914871, mean_q: 553.896132, mean_eps: 0.429404
 64000/100000: episode: 160, duration: 33.213s, episode steps: 400, steps per second:  12, episode reward: 7823.775, mean reward: 19.559 [ 0.000, 57.819], mean action: 1.163 [0.000, 3.000],  loss: 13.138350, mae: 415.222827, mean_q: 556.959387, mean_eps: 0.425804
 64400/100000: episode: 161, duration: 33.378s, episode steps: 400, steps per second:  12, episode reward: 1084.081, mean reward:  2.710 [ 0.000, 23.673], mean action: 1.755 [0.000, 3.000],  loss: 12.722801, mae: 415.989395, mean_q: 557.383561, mean_eps: 0.422204
 64800/100000: episode: 162, duration: 32.918s, episode steps: 400, steps per second:  12, episode reward: 10030.185, mean reward: 25.075 [ 0.000, 96.254], mean action: 1.290 [0.000, 3.000],  loss: 13.481980, mae: 416.903242, mean_q: 558.576217, mean_eps: 0.418604
 65200/100000: episode: 163, duration: 33.383s, episode steps: 400, steps per second:  12, episode reward: 7949.549, mean reward: 19.874 [ 0.000, 101.873], mean action: 1.377 [0.000, 3.000],  loss: 12.191277, mae: 415.460359, mean_q: 556.057926, mean_eps: 0.415004
 65600/100000: episode: 164, duration: 33.287s, episode steps: 400, steps per second:  12, episode reward: 4051.751, mean reward: 10.129 [ 0.000, 69.593], mean action: 1.535 [0.000, 3.000],  loss: 16.605514, mae: 423.913678, mean_q: 564.949756, mean_eps: 0.411405
 66000/100000: episode: 165, duration: 33.178s, episode steps: 400, steps per second:  12, episode reward: 5375.198, mean reward: 13.438 [ 0.000, 57.404], mean action: 1.147 [0.000, 3.000],  loss: 15.680642, mae: 422.011773, mean_q: 564.019383, mean_eps: 0.407804
 66400/100000: episode: 166, duration: 32.757s, episode steps: 400, steps per second:  12, episode reward: 8267.937, mean reward: 20.670 [ 0.000, 79.496], mean action: 1.123 [0.000, 3.000],  loss: 15.690227, mae: 429.039061, mean_q: 573.170493, mean_eps: 0.404205
 66800/100000: episode: 167, duration: 33.199s, episode steps: 400, steps per second:  12, episode reward: 5735.929, mean reward: 14.340 [ 0.000, 73.153], mean action: 1.597 [0.000, 3.000],  loss: 14.182092, mae: 429.464283, mean_q: 575.435266, mean_eps: 0.400605
 67200/100000: episode: 168, duration: 33.096s, episode steps: 400, steps per second:  12, episode reward: 6130.517, mean reward: 15.326 [ 0.000, 60.749], mean action: 1.262 [0.000, 3.000],  loss: 14.814347, mae: 436.080743, mean_q: 585.321557, mean_eps: 0.397005
 67600/100000: episode: 169, duration: 33.640s, episode steps: 400, steps per second:  12, episode reward: 3614.035, mean reward:  9.035 [ 0.000, 54.244], mean action: 1.633 [0.000, 3.000],  loss: 15.130079, mae: 440.788582, mean_q: 589.223687, mean_eps: 0.393404
 68000/100000: episode: 170, duration: 33.085s, episode steps: 400, steps per second:  12, episode reward: 7586.183, mean reward: 18.965 [ 0.000, 102.821], mean action: 1.268 [0.000, 3.000],  loss: 14.117005, mae: 436.623031, mean_q: 584.674826, mean_eps: 0.389805
 68400/100000: episode: 171, duration: 33.382s, episode steps: 400, steps per second:  12, episode reward: 6878.148, mean reward: 17.195 [ 0.000, 101.845], mean action: 1.438 [0.000, 3.000],  loss: 16.854018, mae: 438.282871, mean_q: 585.465709, mean_eps: 0.386205
 68800/100000: episode: 172, duration: 33.387s, episode steps: 400, steps per second:  12, episode reward: 2551.417, mean reward:  6.379 [ 0.000, 45.001], mean action: 1.698 [0.000, 3.000],  loss: 14.602979, mae: 444.016819, mean_q: 594.479920, mean_eps: 0.382604
 69200/100000: episode: 173, duration: 33.079s, episode steps: 400, steps per second:  12, episode reward: 7980.743, mean reward: 19.952 [ 0.000, 94.965], mean action: 1.235 [0.000, 3.000],  loss: 14.257713, mae: 440.779797, mean_q: 589.967893, mean_eps: 0.379004
 69600/100000: episode: 174, duration: 32.947s, episode steps: 400, steps per second:  12, episode reward: 7567.402, mean reward: 18.919 [ 0.000, 89.292], mean action: 1.395 [0.000, 3.000],  loss: 16.118809, mae: 444.781468, mean_q: 594.756727, mean_eps: 0.375404
 70000/100000: episode: 175, duration: 33.033s, episode steps: 400, steps per second:  12, episode reward: 9547.862, mean reward: 23.870 [ 0.000, 100.619], mean action: 1.308 [0.000, 3.000],  loss: 17.922051, mae: 444.162573, mean_q: 593.073706, mean_eps: 0.371804
 70400/100000: episode: 176, duration: 33.080s, episode steps: 400, steps per second:  12, episode reward: 13576.176, mean reward: 33.940 [ 0.000, 116.525], mean action: 1.305 [0.000, 3.000],  loss: 13.844447, mae: 451.089169, mean_q: 604.147619, mean_eps: 0.368204
 70800/100000: episode: 177, duration: 33.197s, episode steps: 400, steps per second:  12, episode reward: 5958.660, mean reward: 14.897 [ 0.000, 56.851], mean action: 1.423 [0.000, 3.000],  loss: 13.162559, mae: 451.524436, mean_q: 603.808235, mean_eps: 0.364604
 71200/100000: episode: 178, duration: 33.404s, episode steps: 400, steps per second:  12, episode reward: 4072.987, mean reward: 10.182 [ 0.000, 70.932], mean action: 1.580 [0.000, 3.000],  loss: 17.411377, mae: 454.861793, mean_q: 607.540441, mean_eps: 0.361004
 71600/100000: episode: 179, duration: 32.999s, episode steps: 400, steps per second:  12, episode reward: 11640.190, mean reward: 29.100 [ 0.000, 101.901], mean action: 1.347 [0.000, 3.000],  loss: 17.578504, mae: 456.116230, mean_q: 609.404330, mean_eps: 0.357404
 72000/100000: episode: 180, duration: 33.312s, episode steps: 400, steps per second:  12, episode reward: 12543.412, mean reward: 31.359 [ 0.000, 101.905], mean action: 1.167 [0.000, 3.000],  loss: 15.221915, mae: 459.581299, mean_q: 615.805108, mean_eps: 0.353804
 72400/100000: episode: 181, duration: 32.985s, episode steps: 400, steps per second:  12, episode reward: 9355.181, mean reward: 23.388 [ 0.000, 62.432], mean action: 1.383 [0.000, 3.000],  loss: 15.530525, mae: 471.111054, mean_q: 631.153296, mean_eps: 0.350204
 72800/100000: episode: 182, duration: 33.185s, episode steps: 400, steps per second:  12, episode reward: 9142.431, mean reward: 22.856 [ 0.000, 101.876], mean action: 1.170 [0.000, 3.000],  loss: 17.090775, mae: 470.187738, mean_q: 628.090455, mean_eps: 0.346604
 73200/100000: episode: 183, duration: 33.144s, episode steps: 400, steps per second:  12, episode reward: 11817.542, mean reward: 29.544 [ 0.000, 98.187], mean action: 1.090 [0.000, 3.000],  loss: 18.828781, mae: 469.399025, mean_q: 627.540881, mean_eps: 0.343004
 73600/100000: episode: 184, duration: 32.857s, episode steps: 400, steps per second:  12, episode reward: 15059.685, mean reward: 37.649 [ 0.000, 130.565], mean action: 1.113 [0.000, 3.000],  loss: 19.645067, mae: 486.950360, mean_q: 651.345818, mean_eps: 0.339404
 74000/100000: episode: 185, duration: 33.059s, episode steps: 400, steps per second:  12, episode reward: 10616.535, mean reward: 26.541 [ 0.000, 73.936], mean action: 1.190 [0.000, 3.000],  loss: 20.846523, mae: 479.072086, mean_q: 638.993517, mean_eps: 0.335805
 74400/100000: episode: 186, duration: 32.888s, episode steps: 400, steps per second:  12, episode reward: 10460.566, mean reward: 26.151 [ 0.000, 96.618], mean action: 1.285 [0.000, 3.000],  loss: 15.506096, mae: 483.778544, mean_q: 647.388024, mean_eps: 0.332205
 74800/100000: episode: 187, duration: 33.065s, episode steps: 400, steps per second:  12, episode reward: 9736.623, mean reward: 24.342 [ 0.000, 91.779], mean action: 1.250 [0.000, 3.000],  loss: 14.860625, mae: 485.303519, mean_q: 650.049630, mean_eps: 0.328605
 75200/100000: episode: 188, duration: 32.982s, episode steps: 400, steps per second:  12, episode reward: 5797.641, mean reward: 14.494 [ 0.000, 102.042], mean action: 1.478 [0.000, 3.000],  loss: 18.046700, mae: 492.063297, mean_q: 656.605752, mean_eps: 0.325005
 75600/100000: episode: 189, duration: 32.997s, episode steps: 400, steps per second:  12, episode reward: 8542.041, mean reward: 21.355 [ 0.000, 101.402], mean action: 1.218 [0.000, 3.000],  loss: 17.623490, mae: 493.865309, mean_q: 659.756371, mean_eps: 0.321405
 76000/100000: episode: 190, duration: 33.044s, episode steps: 400, steps per second:  12, episode reward: 3995.896, mean reward:  9.990 [ 0.000, 58.915], mean action: 1.215 [0.000, 3.000],  loss: 15.825310, mae: 490.160256, mean_q: 655.510361, mean_eps: 0.317805
 76400/100000: episode: 191, duration: 32.885s, episode steps: 400, steps per second:  12, episode reward: 3388.669, mean reward:  8.472 [ 0.000, 42.922], mean action: 1.245 [0.000, 3.000],  loss: 18.347656, mae: 495.149089, mean_q: 660.889677, mean_eps: 0.314204
 76800/100000: episode: 192, duration: 33.226s, episode steps: 400, steps per second:  12, episode reward: 9810.402, mean reward: 24.526 [ 0.000, 85.199], mean action: 1.147 [0.000, 3.000],  loss: 19.916209, mae: 495.648245, mean_q: 661.595057, mean_eps: 0.310604
 77200/100000: episode: 193, duration: 33.189s, episode steps: 400, steps per second:  12, episode reward: 8300.915, mean reward: 20.752 [ 0.000, 101.789], mean action: 1.333 [0.000, 3.000],  loss: 18.313082, mae: 496.313115, mean_q: 664.040756, mean_eps: 0.307005
 77600/100000: episode: 194, duration: 33.651s, episode steps: 400, steps per second:  12, episode reward: 9628.290, mean reward: 24.071 [ 0.000, 101.824], mean action: 1.363 [0.000, 3.000],  loss: 15.431562, mae: 502.562476, mean_q: 672.978743, mean_eps: 0.303404
 78000/100000: episode: 195, duration: 32.905s, episode steps: 400, steps per second:  12, episode reward: 9469.411, mean reward: 23.674 [ 0.000, 78.897], mean action: 1.200 [0.000, 3.000],  loss: 17.530271, mae: 501.518183, mean_q: 670.078555, mean_eps: 0.299804
 78400/100000: episode: 196, duration: 33.152s, episode steps: 400, steps per second:  12, episode reward: 13488.511, mean reward: 33.721 [ 0.000, 113.638], mean action: 1.405 [0.000, 3.000],  loss: 17.152692, mae: 509.504537, mean_q: 682.749688, mean_eps: 0.296204
 78800/100000: episode: 197, duration: 32.846s, episode steps: 400, steps per second:  12, episode reward: 7560.632, mean reward: 18.902 [ 0.000, 64.907], mean action: 1.285 [0.000, 3.000],  loss: 20.281193, mae: 505.088283, mean_q: 674.039507, mean_eps: 0.292604
 79200/100000: episode: 198, duration: 33.184s, episode steps: 400, steps per second:  12, episode reward: 6127.232, mean reward: 15.318 [ 0.000, 78.933], mean action: 1.420 [0.000, 3.000],  loss: 15.242930, mae: 508.147345, mean_q: 679.425738, mean_eps: 0.289004
 79600/100000: episode: 199, duration: 33.250s, episode steps: 400, steps per second:  12, episode reward: 16020.965, mean reward: 40.052 [ 0.000, 104.838], mean action: 1.160 [0.000, 3.000],  loss: 22.069753, mae: 509.316248, mean_q: 681.091737, mean_eps: 0.285405
 80000/100000: episode: 200, duration: 33.161s, episode steps: 400, steps per second:  12, episode reward: 10666.874, mean reward: 26.667 [ 0.000, 101.970], mean action: 1.605 [0.000, 3.000],  loss: 16.300137, mae: 513.404009, mean_q: 686.992936, mean_eps: 0.281805
 80400/100000: episode: 201, duration: 33.000s, episode steps: 400, steps per second:  12, episode reward: 14581.660, mean reward: 36.454 [ 0.000, 126.918], mean action: 1.160 [0.000, 3.000],  loss: 17.922330, mae: 514.003912, mean_q: 686.746240, mean_eps: 0.278205
 80800/100000: episode: 202, duration: 33.070s, episode steps: 400, steps per second:  12, episode reward: 15166.400, mean reward: 37.916 [ 0.000, 113.282], mean action: 1.150 [0.000, 3.000],  loss: 19.349535, mae: 519.562294, mean_q: 694.837171, mean_eps: 0.274604
 81200/100000: episode: 203, duration: 32.970s, episode steps: 400, steps per second:  12, episode reward: 10749.458, mean reward: 26.874 [ 0.000, 104.627], mean action: 1.548 [0.000, 3.000],  loss: 16.700304, mae: 519.947389, mean_q: 695.633126, mean_eps: 0.271004
 81600/100000: episode: 204, duration: 32.796s, episode steps: 400, steps per second:  12, episode reward: 8002.528, mean reward: 20.006 [ 0.000, 101.849], mean action: 1.133 [0.000, 3.000],  loss: 17.328542, mae: 521.940780, mean_q: 697.678204, mean_eps: 0.267404
 82000/100000: episode: 205, duration: 32.963s, episode steps: 400, steps per second:  12, episode reward: 12498.886, mean reward: 31.247 [ 0.000, 101.841], mean action: 1.230 [0.000, 3.000],  loss: 14.516520, mae: 521.054228, mean_q: 698.577106, mean_eps: 0.263804
 82400/100000: episode: 206, duration: 32.925s, episode steps: 400, steps per second:  12, episode reward: 11424.370, mean reward: 28.561 [ 0.000, 78.125], mean action: 1.113 [0.000, 3.000],  loss: 13.528745, mae: 524.654015, mean_q: 702.781668, mean_eps: 0.260204
 82800/100000: episode: 207, duration: 33.040s, episode steps: 400, steps per second:  12, episode reward: 9076.249, mean reward: 22.691 [ 0.000, 96.596], mean action: 1.548 [0.000, 3.000],  loss: 17.825336, mae: 525.551285, mean_q: 702.807763, mean_eps: 0.256604
 83200/100000: episode: 208, duration: 32.967s, episode steps: 400, steps per second:  12, episode reward: 15215.140, mean reward: 38.038 [ 0.000, 125.215], mean action: 1.177 [0.000, 3.000],  loss: 18.142433, mae: 528.795415, mean_q: 707.129663, mean_eps: 0.253004
 83600/100000: episode: 209, duration: 33.285s, episode steps: 400, steps per second:  12, episode reward: 15060.233, mean reward: 37.651 [ 0.000, 117.320], mean action: 1.065 [0.000, 3.000],  loss: 18.003725, mae: 530.844123, mean_q: 711.982991, mean_eps: 0.249404
 84000/100000: episode: 210, duration: 33.181s, episode steps: 400, steps per second:  12, episode reward: 8049.484, mean reward: 20.124 [ 0.000, 89.208], mean action: 1.320 [0.000, 3.000],  loss: 20.215559, mae: 536.237803, mean_q: 716.293257, mean_eps: 0.245804
 84400/100000: episode: 211, duration: 33.048s, episode steps: 400, steps per second:  12, episode reward: 6995.740, mean reward: 17.489 [ 0.000, 53.917], mean action: 1.540 [0.000, 3.000],  loss: 20.192728, mae: 542.811967, mean_q: 725.423087, mean_eps: 0.242204
 84800/100000: episode: 212, duration: 33.965s, episode steps: 400, steps per second:  12, episode reward: 2037.440, mean reward:  5.094 [ 0.000, 44.088], mean action: 1.502 [0.000, 3.000],  loss: 19.155780, mae: 539.822014, mean_q: 722.253636, mean_eps: 0.238604
 85200/100000: episode: 213, duration: 32.905s, episode steps: 400, steps per second:  12, episode reward: 15417.578, mean reward: 38.544 [ 0.000, 120.470], mean action: 1.000 [0.000, 3.000],  loss: 20.277817, mae: 541.356244, mean_q: 725.389667, mean_eps: 0.235005
 85341/100000: episode: 214, duration: 15.581s, episode steps: 141, steps per second:   9, episode reward: 38623.108, mean reward: 273.923 [ 0.000, 25968.051], mean action: 0.496 [0.000, 3.000],  loss: 15.309568, mae: 553.554609, mean_q: 743.317217, mean_eps: 0.232570
 85741/100000: episode: 215, duration: 33.138s, episode steps: 400, steps per second:  12, episode reward: 8305.591, mean reward: 20.764 [ 0.000, 101.836], mean action: 1.635 [0.000, 3.000],  loss: 15.834302, mae: 544.471881, mean_q: 729.543937, mean_eps: 0.230135
 86141/100000: episode: 216, duration: 32.872s, episode steps: 400, steps per second:  12, episode reward: 11540.462, mean reward: 28.851 [ 0.000, 95.351], mean action: 1.170 [0.000, 3.000],  loss: 17.197180, mae: 548.214439, mean_q: 733.566843, mean_eps: 0.226536
 86541/100000: episode: 217, duration: 33.015s, episode steps: 400, steps per second:  12, episode reward: 14833.014, mean reward: 37.083 [ 0.000, 134.281], mean action: 1.110 [0.000, 3.000],  loss: 21.099517, mae: 553.287735, mean_q: 739.716926, mean_eps: 0.222935
 86941/100000: episode: 218, duration: 33.141s, episode steps: 400, steps per second:  12, episode reward: 10401.292, mean reward: 26.003 [ 0.000, 101.707], mean action: 1.445 [0.000, 3.000],  loss: 19.662996, mae: 552.604607, mean_q: 738.360255, mean_eps: 0.219336
 87341/100000: episode: 219, duration: 32.909s, episode steps: 400, steps per second:  12, episode reward: 13121.649, mean reward: 32.804 [ 0.000, 105.358], mean action: 1.472 [0.000, 3.000],  loss: 21.499106, mae: 556.442755, mean_q: 742.529363, mean_eps: 0.215735
 87741/100000: episode: 220, duration: 33.104s, episode steps: 400, steps per second:  12, episode reward: 7033.711, mean reward: 17.584 [ 0.000, 74.182], mean action: 1.585 [0.000, 3.000],  loss: 18.397978, mae: 556.471738, mean_q: 745.168816, mean_eps: 0.212135
 87856/100000: episode: 221, duration: 13.425s, episode steps: 115, steps per second:   9, episode reward: 41125.510, mean reward: 357.613 [ 0.000, 28677.920], mean action: 0.252 [0.000, 3.000],  loss: 17.394880, mae: 552.759814, mean_q: 742.549990, mean_eps: 0.209818
 88256/100000: episode: 222, duration: 33.246s, episode steps: 400, steps per second:  12, episode reward: 13494.687, mean reward: 33.737 [ 0.000, 93.705], mean action: 1.360 [0.000, 3.000],  loss: 17.078309, mae: 556.845195, mean_q: 744.523955, mean_eps: 0.207500
 88656/100000: episode: 223, duration: 32.874s, episode steps: 400, steps per second:  12, episode reward: 4293.019, mean reward: 10.733 [ 0.000, 71.137], mean action: 1.288 [0.000, 3.000],  loss: 21.275196, mae: 561.345324, mean_q: 748.716915, mean_eps: 0.203900
 89042/100000: episode: 224, duration: 35.189s, episode steps: 386, steps per second:  11, episode reward: 16288.806, mean reward: 42.199 [ 0.000, 1524.160], mean action: 0.899 [0.000, 3.000],  loss: 17.229564, mae: 565.169545, mean_q: 757.957915, mean_eps: 0.200363
 89442/100000: episode: 225, duration: 32.826s, episode steps: 400, steps per second:  12, episode reward: 13355.533, mean reward: 33.389 [ 0.000, 107.529], mean action: 1.150 [0.000, 3.000],  loss: 18.838680, mae: 581.067486, mean_q: 778.550970, mean_eps: 0.196826
 89842/100000: episode: 226, duration: 33.043s, episode steps: 400, steps per second:  12, episode reward: 12176.182, mean reward: 30.440 [ 0.000, 101.745], mean action: 1.080 [0.000, 3.000],  loss: 19.280741, mae: 574.283883, mean_q: 767.407235, mean_eps: 0.193226
 90242/100000: episode: 227, duration: 32.888s, episode steps: 400, steps per second:  12, episode reward: 11838.629, mean reward: 29.597 [ 0.000, 101.831], mean action: 0.855 [0.000, 3.000],  loss: 18.502554, mae: 583.067088, mean_q: 781.278782, mean_eps: 0.189627
 90642/100000: episode: 228, duration: 33.145s, episode steps: 400, steps per second:  12, episode reward: 9766.139, mean reward: 24.415 [ 0.000, 108.011], mean action: 1.015 [0.000, 3.000],  loss: 18.557975, mae: 589.254713, mean_q: 787.767662, mean_eps: 0.186026
 91042/100000: episode: 229, duration: 33.205s, episode steps: 400, steps per second:  12, episode reward: 13247.513, mean reward: 33.119 [ 0.000, 104.319], mean action: 1.205 [0.000, 3.000],  loss: 17.867481, mae: 584.371418, mean_q: 780.920161, mean_eps: 0.182426
 91442/100000: episode: 230, duration: 33.724s, episode steps: 400, steps per second:  12, episode reward: 8019.331, mean reward: 20.048 [ 0.000, 73.767], mean action: 1.407 [0.000, 3.000],  loss: 25.184496, mae: 594.051714, mean_q: 793.131104, mean_eps: 0.178826
 91842/100000: episode: 231, duration: 33.170s, episode steps: 400, steps per second:  12, episode reward: 15296.036, mean reward: 38.240 [ 0.000, 124.853], mean action: 1.028 [0.000, 3.000],  loss: 20.516115, mae: 590.584820, mean_q: 789.026531, mean_eps: 0.175226
 92242/100000: episode: 232, duration: 32.971s, episode steps: 400, steps per second:  12, episode reward: 6900.484, mean reward: 17.251 [ 0.000, 83.635], mean action: 1.580 [0.000, 3.000],  loss: 19.403259, mae: 597.962579, mean_q: 799.835573, mean_eps: 0.171626
 92376/100000: episode: 233, duration: 15.073s, episode steps: 134, steps per second:   9, episode reward: 39104.100, mean reward: 291.822 [ 0.000, 26726.991], mean action: 0.366 [0.000, 3.000],  loss: 19.686945, mae: 594.236667, mean_q: 795.561967, mean_eps: 0.169223
 92776/100000: episode: 234, duration: 33.251s, episode steps: 400, steps per second:  12, episode reward: 10590.868, mean reward: 26.477 [ 0.000, 113.800], mean action: 1.163 [0.000, 3.000],  loss: 18.581493, mae: 594.513027, mean_q: 795.565395, mean_eps: 0.166820
 93176/100000: episode: 235, duration: 32.955s, episode steps: 400, steps per second:  12, episode reward: 11672.207, mean reward: 29.181 [ 0.000, 70.877], mean action: 1.235 [0.000, 3.000],  loss: 16.876278, mae: 594.201116, mean_q: 795.720510, mean_eps: 0.163220
 93576/100000: episode: 236, duration: 33.129s, episode steps: 400, steps per second:  12, episode reward: 11039.657, mean reward: 27.599 [ 0.000, 136.852], mean action: 1.200 [0.000, 3.000],  loss: 26.559802, mae: 591.448954, mean_q: 787.240763, mean_eps: 0.159620
 93891/100000: episode: 237, duration: 29.446s, episode steps: 315, steps per second:  11, episode reward: 23801.421, mean reward: 75.560 [ 0.000, 8651.619], mean action: 0.848 [0.000, 3.000],  loss: 22.115341, mae: 598.512098, mean_q: 800.060516, mean_eps: 0.156403
 94209/100000: episode: 238, duration: 29.810s, episode steps: 318, steps per second:  11, episode reward: 25517.686, mean reward: 80.244 [ 0.000, 8320.150], mean action: 0.799 [0.000, 3.000],  loss: 22.703635, mae: 603.319601, mean_q: 807.047970, mean_eps: 0.153554
 94593/100000: episode: 239, duration: 34.677s, episode steps: 384, steps per second:  11, episode reward: 16963.829, mean reward: 44.177 [ 0.000, 1673.604], mean action: 0.768 [0.000, 3.000],  loss: 20.738564, mae: 606.148615, mean_q: 810.700743, mean_eps: 0.150395
 94993/100000: episode: 240, duration: 33.243s, episode steps: 400, steps per second:  12, episode reward: 13213.332, mean reward: 33.033 [ 0.000, 119.200], mean action: 1.315 [0.000, 3.000],  loss: 20.474567, mae: 608.773714, mean_q: 815.321607, mean_eps: 0.146867
 95393/100000: episode: 241, duration: 33.178s, episode steps: 400, steps per second:  12, episode reward: 13285.775, mean reward: 33.214 [ 0.000, 106.553], mean action: 1.052 [0.000, 3.000],  loss: 24.812188, mae: 609.773392, mean_q: 817.125144, mean_eps: 0.143267
 95793/100000: episode: 242, duration: 33.115s, episode steps: 400, steps per second:  12, episode reward: 18260.058, mean reward: 45.650 [ 0.000, 117.592], mean action: 0.985 [0.000, 3.000],  loss: 24.973222, mae: 612.193056, mean_q: 818.491576, mean_eps: 0.139667
 96193/100000: episode: 243, duration: 33.036s, episode steps: 400, steps per second:  12, episode reward: 18717.031, mean reward: 46.793 [ 0.000, 162.403], mean action: 1.030 [0.000, 3.000],  loss: 20.638331, mae: 609.780204, mean_q: 815.716084, mean_eps: 0.136067
 96593/100000: episode: 244, duration: 33.027s, episode steps: 400, steps per second:  12, episode reward: 9276.536, mean reward: 23.191 [ 0.000, 114.076], mean action: 1.397 [0.000, 3.000],  loss: 20.743271, mae: 609.764793, mean_q: 816.130181, mean_eps: 0.132467
 96716/100000: episode: 245, duration: 14.216s, episode steps: 123, steps per second:   9, episode reward: 40288.432, mean reward: 327.548 [ 0.000, 27826.577], mean action: 0.512 [0.000, 3.000],  loss: 22.589822, mae: 614.707829, mean_q: 823.506319, mean_eps: 0.130114
 97116/100000: episode: 246, duration: 33.305s, episode steps: 400, steps per second:  12, episode reward: 7556.494, mean reward: 18.891 [ 0.000, 90.733], mean action: 1.778 [0.000, 3.000],  loss: 21.724172, mae: 619.650868, mean_q: 828.583979, mean_eps: 0.127760
 97516/100000: episode: 247, duration: 33.048s, episode steps: 400, steps per second:  12, episode reward: 15461.047, mean reward: 38.653 [ 0.000, 124.753], mean action: 0.858 [0.000, 3.000],  loss: 30.226492, mae: 618.570806, mean_q: 823.974636, mean_eps: 0.124160
 97916/100000: episode: 248, duration: 33.546s, episode steps: 400, steps per second:  12, episode reward: 3756.746, mean reward:  9.392 [ 0.000, 63.053], mean action: 1.758 [0.000, 3.000],  loss: 25.493147, mae: 629.750251, mean_q: 841.109553, mean_eps: 0.120560
 98316/100000: episode: 249, duration: 33.047s, episode steps: 400, steps per second:  12, episode reward: 8735.923, mean reward: 21.840 [ 0.000, 110.382], mean action: 1.222 [0.000, 3.000],  loss: 25.335086, mae: 623.489966, mean_q: 833.000390, mean_eps: 0.116960
 98716/100000: episode: 250, duration: 33.702s, episode steps: 400, steps per second:  12, episode reward: 3285.855, mean reward:  8.215 [ 0.000, 72.680], mean action: 1.808 [0.000, 3.000],  loss: 21.891638, mae: 631.054262, mean_q: 845.410419, mean_eps: 0.113360
 99116/100000: episode: 251, duration: 33.468s, episode steps: 400, steps per second:  12, episode reward: 13726.124, mean reward: 34.315 [ 0.000, 85.309], mean action: 0.830 [0.000, 3.000],  loss: 24.171444, mae: 634.037473, mean_q: 848.943790, mean_eps: 0.109760
 99499/100000: episode: 252, duration: 34.952s, episode steps: 383, steps per second:  11, episode reward: 15600.923, mean reward: 40.733 [ 0.000, 1874.935], mean action: 1.710 [0.000, 3.000],  loss: 25.836209, mae: 636.920350, mean_q: 853.371988, mean_eps: 0.106237
 99726/100000: episode: 253, duration: 22.392s, episode steps: 227, steps per second:  10, episode reward: 31110.120, mean reward: 137.049 [ 0.000, 17411.821], mean action: 0.617 [0.000, 3.000],  loss: 23.812060, mae: 640.522386, mean_q: 857.257292, mean_eps: 0.103492
done, took 8335.877 seconds
